{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project 1: Airline Flight\n",
    "We will develope a model which predice whether or not a given flight will be delayed.  \n",
    "\n",
    "We're going to create the stages for the flights duration model pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n",
      "The data contain 50000 records.\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|    km|\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+\n",
      "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30| 509.0|\n",
      "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8| 542.0|\n",
      "|  9| 13|  1|     AA|   419|ORD| 10.33|     195|   -5|1989.0|\n",
      "|  5|  2|  1|     UA|   704|SFO|  7.98|     102|    2| 885.0|\n",
      "|  7|  2|  6|     AA|   380|ORD| 10.83|     135|   54|1180.0|\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mon', 'int'),\n",
       " ('dom', 'int'),\n",
       " ('dow', 'int'),\n",
       " ('carrier', 'string'),\n",
       " ('flight', 'int'),\n",
       " ('org', 'string'),\n",
       " ('depart', 'double'),\n",
       " ('duration', 'int'),\n",
       " ('delay', 'int'),\n",
       " ('km', 'double')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('C:/Users/Hamid/Anaconda3/lib/site-packages/pyspark')\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder.master('local[*]').appName('test').getOrCreate()\n",
    "\n",
    "# What version of Spark?\n",
    "print(spark.version)\n",
    "\n",
    "# Read data from CSV file, inferSchema - deduce column data types from data.\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights = flights.filter('delay IS NOT NULL')\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use cross validation to choose an optimal (or close to optimal) set of model hyper-parameters.\n",
    "- elasticNetParam is the combination of L1 and L2 regularization\n",
    "- regParam is the landa in regularization \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  12\n",
      "[StringIndexer_f3a9130ace79, OneHotEncoderEstimator_da59a617c844, VectorAssembler_09ad17235c0c, LinearRegression_c25edd9ea78b]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.222357206540181"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoderEstimator( inputCols=['org_idx', 'dow'], outputCols=['org_dummy', 'dow_dummy'])\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'dow_dummy'], outputCol='features')\n",
    "\n",
    "# A linear regression object\n",
    "regression = LinearRegression(labelCol='duration')\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "\n",
    "\n",
    "# Construct a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grids for two parameters\n",
    "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
    "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "\n",
    "\n",
    "# Build the parameter grid\n",
    "params = params.build()\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "cv = cv.fit(flights_train)\n",
    "\n",
    "\n",
    "# Get the best model from cross validation\n",
    "best_model = cv.bestModel\n",
    "\n",
    "# Look at the stages in the best model\n",
    "print(best_model.stages)\n",
    "\n",
    "# Get the parameters for the LinearRegression object in the best model\n",
    "best_model.stages[3].extractParamMap()\n",
    "\n",
    "# Generate predictions on testing data using the best model then calculate RMSE\n",
    "predictions = best_model.transform(flights_test)\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models: \n",
    "\n",
    "GBTClassifier : Gradientboosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-----+-----------------+\n",
      "|mon|depart|duration|label|features         |\n",
      "+---+------+--------+-----+-----------------+\n",
      "|0  |16.33 |82      |1    |[0.0,16.33,82.0] |\n",
      "|2  |6.17  |82      |0    |[2.0,6.17,82.0]  |\n",
      "|9  |10.33 |195     |0    |[9.0,10.33,195.0]|\n",
      "|5  |7.98  |102     |0    |[5.0,7.98,102.0] |\n",
      "|7  |10.83 |135     |1    |[7.0,10.83,135.0]|\n",
      "+---+------+--------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "flights = flights.withColumn('label', (flights.delay >= 15).cast('integer'))\n",
    "\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=['mon','depart', 'duration'], outputCol='features')\n",
    "\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights).drop('km','dom','dow', 'carrier', 'flight','org', 'delay')\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "(3,[0,1,2],[0.3362368880567573,0.28840695395646115,0.3753561579867815])\n"
     ]
    }
   ],
   "source": [
    "# Import the classes required\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "\n",
    "# Create model objects and train on training data\n",
    "tree = DecisionTreeClassifier().fit(flights_train)\n",
    "gbt = GBTClassifier().fit(flights_train)\n",
    "\n",
    "# Compare AUC on testing data\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(tree.transform(flights_test))\n",
    "evaluator.evaluate(gbt.transform(flights_test))\n",
    "\n",
    "# Find the number of trees and the relative importance of features\n",
    "print(gbt.getNumTrees)\n",
    "print(gbt.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
